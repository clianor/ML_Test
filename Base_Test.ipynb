{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukhyun\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")  # window에서 Mecab을 사용하기 위해 설정 후 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "def Data_Read(path):\n",
    "    with codecs.open(path, 'r', 'utf-8') as f:\n",
    "        docs = f.read()\n",
    "    bs = BeautifulSoup(docs, \"lxml\")\n",
    "    docsList = bs.find_all('c')\n",
    "    \n",
    "    # 데이터 \n",
    "    docs = []\n",
    "    for doc in docsList:\n",
    "        cited = doc.get('citedarticle')\n",
    "        citing = doc.get('citingarticle')\n",
    "        clue = doc.get('clue')\n",
    "        sentiment = doc.get('sentiment')\n",
    "        sentiments = [str(l.text) for l in doc.find_all('s')] # l.text로 인해 target tag 역시 사라집니다.\n",
    "        docs.append([cited, citing, sentiment, clue, ' '.join(sentiments)])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = Data_Read('Citation_Data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clue, Sentiments, Sentiment로 데이터 재정리\n",
    "pnn = {'POS':2, 'NEG':1, 'NEU':0, 'NEU,NEU':0}\n",
    "X = np.array([l[4] for l in docs])\n",
    "Y = np.array([pnn[l[2]] for l in docs])\n",
    "clue = np.array([l[3] for l in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukhyun\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "               precision      recall    f1-score\n",
      "         NEU        0.71        0.92        0.80\n",
      "         NEG        0.03        0.01        0.02\n",
      "         POS        0.13        0.04        0.06\n",
      " avg / total        0.53        0.66        0.58\n",
      "\n",
      "Naive Bayes\n",
      "               precision      recall    f1-score\n",
      "         NEU        0.79        0.16        0.26\n",
      "         NEG        0.11        0.73        0.19\n",
      "         POS        0.17        0.14        0.15\n",
      " avg / total        0.61        0.22        0.23\n",
      "\n",
      "Logistic Regression\n",
      "               precision      recall    f1-score\n",
      "         NEU        0.71        0.96        0.82\n",
      "         NEG        0.06        0.03        0.04\n",
      "         POS        0.17        0.02        0.04\n",
      " avg / total        0.55        0.70        0.60\n",
      "\n",
      "KNN Classification\n",
      "               precision      recall    f1-score\n",
      "         NEU        0.72        0.82        0.74\n",
      "         NEG        0.05        0.13        0.05\n",
      "         POS        0.28        0.05        0.08\n",
      " avg / total        0.57        0.61        0.55\n",
      "\n",
      "DecisionTree\n",
      "               precision      recall    f1-score\n",
      "         NEU        0.71        0.97        0.82\n",
      "         NEG        0.00        0.00        0.00\n",
      "         POS        0.02        0.01        0.01\n",
      " avg / total        0.51        0.69        0.59\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 4/5는 train set, 1/5는 test set으로 이용합니다.(비율을 맞춰 나눕니다.)\n",
    "skf = StratifiedKFold(Y, n_folds=5, shuffle=True)\n",
    "nb = []\n",
    "svm = []\n",
    "lg = []\n",
    "knn = []\n",
    "dt = []\n",
    "pnlist = ['SY', 'SSO', 'SC'] # 제거할 형태소\n",
    "for train, test in skf:\n",
    "    trainX, trainY, trainClue = X[train], Y[train], clue[train]\n",
    "    testX, testY, testClue = X[test], Y[test], clue[test]\n",
    "    \n",
    "    '''\n",
    "    형태소 분석기는 Mecab을 이용합니다.\n",
    "    Train Clue를 단어 Feature로 사용하기 위하여 형태소 분석을하여 단어 사전을 구축합니다.\n",
    "    이 부분에서 추출한 feature가 학습시 이용됩니다.\n",
    "    '''\n",
    "    word_dict = []\n",
    "    clue_dict = []\n",
    "    if True: # 형태소를 feature로 사용합니다.\n",
    "        for doc in trainX:\n",
    "            word_dict += [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if not i[1] in pnlist]\n",
    "        word_dict = set(word_dict)\n",
    "        word_dict = {w:i for i, w in enumerate(word_dict)}\n",
    "    elif False: # clue를 형태소 분석하여 feature로 사용합니다.\n",
    "        for c in trainClue:\n",
    "            if c:\n",
    "                word_dict += [i[0]+'/'+i[1] for i in mecab.pos(c.replace(' ', '')) if not i[1] in pnlist]\n",
    "        word_dict = set(word_dict)\n",
    "        word_dict = {w:i for i, w in enumerate(word_dict)}\n",
    "            \n",
    "    '''\n",
    "    clue 형태소 분석 clue_dict 만들기\n",
    "    '''\n",
    "    if True:\n",
    "        for c in trainClue:\n",
    "            if c:\n",
    "                clue_dict += [i[0]+'/'+i[1] for i in mecab.pos(c.replace(' ', '')) if not i[1] in pnlist]\n",
    "        clue_dict = set(clue_dict)\n",
    "        clue_dict = {w:i for i, w in enumerate(word_dict)}\n",
    "    \n",
    "    \n",
    "    # 데이터를 넣을 공간을 만듭니다.\n",
    "    train_X = np.zeros((len(trainY), len(word_dict)), dtype=np.int32)\n",
    "    test_X = np.zeros((len(testY), len(word_dict)), dtype=np.int32)\n",
    "    \n",
    "    # 데이터들의 문장을 띄어쓰기를 제거하고 형태소 분석하고, 기타 기호 SY 형태소를 제거하였음\n",
    "    # 띄어쓰기를 제거한 이유는 다음과 같습니다.\n",
    "    # 문장을 읽어올때 잘못된 띄어쓰기 문제가 발생하였기 때문에 띄어쓰기를 제거하여 형태소 분석을 합니다. \n",
    "    # Train Set\n",
    "    for i, doc in enumerate(trainX):\n",
    "        doc = [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if not i[1] in pnlist]\n",
    "        for w in doc:\n",
    "            try:\n",
    "                train_X[word_dict[w], i] += 1\n",
    "                if w in clue_dict:\n",
    "                    train_X[word_dict[w], i] += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Test Set\n",
    "    for i, doc in enumerate(testX):\n",
    "        doc = [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if not i[1] in pnlist]\n",
    "        for w in doc:\n",
    "            try:\n",
    "                test_X[word_dict[w], i] += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # NaiveBayes\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(train_X, trainY)\n",
    "    nb.append(classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']).split())\n",
    "    \n",
    "    # SVM Classification\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(train_X, trainY)\n",
    "    svm.append(np.array(classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']).split()))\n",
    "    \n",
    "    # LogisticRegression\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_X, trainY)\n",
    "    lg.append(np.array(classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']).split()))\n",
    "    \n",
    "    # KNN = 5\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(train_X, trainY)\n",
    "    knn.append(np.array(classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']).split()))\n",
    "    \n",
    "    # dt\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(train_X, trainY)\n",
    "    dt.append(np.array(classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']).split()))\n",
    "    \n",
    "\n",
    "def mean(np_array):\n",
    "    arr = []\n",
    "    for i in range(np_array.shape[1]):\n",
    "        arr.append(np.mean(np_array[:,i]))\n",
    "    return np.round(np.array(arr), 2)\n",
    "    \n",
    "def data_report(array):\n",
    "    NEU = np.array(array)[:, 5:][:,:3].astype(dtype=np.float32)\n",
    "    NEG = np.array(array)[:, 5:][:,5:8].astype(dtype=np.float32)\n",
    "    POS = np.array(array)[:, 5:][:,10:13].astype(dtype=np.float32)\n",
    "    avg_total = np.array(array)[:,5:][:,17:20].astype(dtype=np.float32)\n",
    "    print(\"%24s%12s%12s\" %(\"precision\", \"recall\", \"f1-score\"))\n",
    "    a, b, c = mean(NEU)\n",
    "    print(\"%12s        %0.2f        %0.2f        %0.2f\" %(\"NEU\", a, b, c))\n",
    "    a, b, c = mean(NEG)\n",
    "    print(\"%12s        %0.2f        %0.2f        %0.2f\" %(\"NEG\", a, b, c))\n",
    "    a, b, c = mean(POS)\n",
    "    print(\"%12s        %0.2f        %0.2f        %0.2f\" %(\"POS\", a, b, c))\n",
    "    a, b, c = mean(avg_total)\n",
    "    print(\"%12s        %0.2f        %0.2f        %0.2f\" %(\"avg / total\", a, b, c))\n",
    "\n",
    "print(\"Linear SVM\")\n",
    "data_report(svm)\n",
    "\n",
    "print(\"\\nNaive Bayes\")\n",
    "data_report(nb)\n",
    "\n",
    "print(\"\\nLogistic Regression\")\n",
    "data_report(lg)\n",
    "\n",
    "print(\"\\nKNN Classification\")\n",
    "data_report(knn)\n",
    "\n",
    "print(\"\\nDecisionTree\")\n",
    "data_report(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
